@book{Miner2012,
abstract = {MapReduce Design Patterns},
address = {Sebastopol},
author = {Miner, Donald and Shook, Adam},
edition = {First Edit},
editor = {Oram, Andy and Hendrickson, Mike},
file = {:C$\backslash$:/Users/Jake/Documents/Thesis References/mapreduce\_design\_patterns.pdf:pdf},
isbn = {9781449327170},
pages = {251},
publisher = {O'Reilly},
title = {{MapReduce Design Patterns}},
year = {2012}
}
@misc{Blake1998,
author = {Blake, C and Merz, C},
title = {{UCI Repository of machine learning databases}},
year = {1998}
}
@article{Hahsler2011,
abstract = {Association rulemining is a popular dataminingmethod available in R as the extension package˜arules. However, mining association rules often results in a very large number of found rules, leaving the analyst with the task to go through all the rules and discover interesting ones. Sifting manually through large sets of rules is time consuming and strenuous. Visualization has a long history of making large data sets better accessible using techniques like selecting and zooming. In this paper we present the R-extension package˜arulesViz which implements several known and novel visualization techniques to explore association rules. With examples we show how these visualization techniques can be used to analyze a data set.},
author = {Hahsler, Michael and Chelluboina, S},
file = {:C$\backslash$:/Users/Jake/Documents/Thesis References/arulesViz.pdf:pdf},
journal = {R project module},
keywords = {association rules,data mining,visualization},
title = {{Visualizing Association Rules: Introduction to the R-extension Package arulesViz}},
url = {http://www.comp.nus.edu.sg/~zhanghao/project/visualization/[2010]arulesViz.pdf},
year = {2011}
}
@inproceedings{kohavi1996,
author = {Kohavi, Ron},
booktitle = {KDD-96 Proceedings},
file = {:C$\backslash$:/Users/Jake/Documents/Thesis References/kohavi.pdf:pdf},
pages = {202--207},
title = {{Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid}},
year = {1996}
}
@article{Fisher1936,
abstract = {When two or more populations have been measured in several characters, special interest attaches to certain functions of the measurements by which the populations are discriminated. At the author's suggestion use has already been made of this fact in craniometry (a) by Mr E. S. Martin, who has applied the principle to the sex differences in measurements of the mandible, and (b) by Miss Mildred Barnard, who showed how to obtain from a series of dated series the particular compound of cranial measurements showing most distinctly a progressive or secular trend. In the present paper the application of the same principle will be illustrated on a taxonomic problem; some questions connected with the precision of the processes employed will also be discussed.},
author = {Fisher, Ra},
doi = {10.1111/j.1469-1809.1936.tb02137.x},
file = {:C$\backslash$:/Users/Jake/Documents/Thesis References/Fisher\_iris.pdf:pdf},
isbn = {1469-1809},
issn = {1469-1809},
journal = {Annals of Eugenics},
keywords = {Iris data set,LDA},
number = {2},
pages = {179--188},
pmid = {334},
title = {{The Use of Multiple Measurements in Taxonomic Problems}},
volume = {7},
year = {1936}
}
@article{Woo,
author = {Woo, Jongwook},
file = {:C$\backslash$:/Users/Jake/Documents/Thesis References/Apriori\_MapReduce.pdf:pdf},
keywords = {11-13,algorithms with map,apriori algorithm,association rule,cloud computing,data mining,hadoop,map,presents market basket analysis,reduce,which proposes the algorithm,woo et al},
title = {{Apriori-Map / Reduce Algorithm}}
}
@inproceedings{Yang2010,
abstract = {As association rules widely used, it needs to study many problems, one of which is the generally larger and multi-dimensional datasets, and the rapid growth of the mount of data. Single-processor's memory and CPU resources are very limited, which makes the algorithm performance inefficient. Recently the development of network and distributed technology makes cloud computing a reality in the implementation of association rules algorithm. In this paper we describe the improved Apriori algorithm based on MapReduce mode, which can handle massive datasets with a large number of nodes on Hadoop platform.},
author = {Yang, Xin Yue and Liu, Zhen and Fu, Yan},
booktitle = {Proceedings - 3rd International Conference on Information Sciences and Interaction Sciences, ICIS 2010},
doi = {10.1109/ICICIS.2010.5534718},
file = {:C$\backslash$:/Users/Jake/Documents/Thesis References/Apriori\_Hadoop.pdf:pdf},
isbn = {9781424473854},
keywords = {Apriori,Association rules,Hadoop,KDD,MapReduce},
pages = {99--102},
title = {{MapReduce as a programming model for association rules algorithm on Hadoop}},
year = {2010}
}
@article{Gedik2008,
abstract = {In this paper, we present Spade − the System S declarative stream processing engine. System S is a large-scale, dis- tributed data stream processing middleware under develop- ment at IBM T. J.Watson Research Center. As a front-end for rapid application development for System S, Spade pro- vides (1) an intermediate language for flexible composition of parallel and distributed data-flow graphs, (2) a toolkit of type-generic, built-in stream processing operators, that sup- port scalar as well as vectorized processing and can seam- lessly inter-operate with user-defined operators, and (3) a rich set of stream adapters to ingest/publish data from/to outside sources. More importantly, Spade automatically brings performance optimization and scalability to System S applications. To that end, Spade employs a code genera- tion framework to create highly-optimized applications that run natively on the Stream Processing Core (SPC), the exe- cution and communication substrate of System S, and take full advantage of other System S services. Spade allows de- velopers to construct their applications with fine granular stream operators without worrying about the performance implications that might exist, even in a distributed system. Spade’s optimizing compiler automatically maps applica- tions into appropriately sized execution units in order to minimize communication overhead, while at the same time exploiting available parallelism. By virtue of the scalability of the System S runtime and Spade’s effective code genera- tion and optimization, we can scale applications to a large number of nodes. Currently, we can run Spade jobs on ≈ 500 processors within more than 100 physical nodes in a tightly connected cluster environment. Spade has been in use at IBM Research to create real-world streaming applications, ranging from monitoring financial market feeds to radio telescopes to semiconductor fabrication lines. Categories},
author = {Gedik, B ( 1 ) and Andrade, H ( 1 ) and Wu, K.-L. ( 1 ) and Yu, P S ( 2 ) and Doo, M ( 3 )},
doi = {10.1145/1376616.1376729},
file = {:C$\backslash$:/Users/Jake/Documents/Thesis References/gedik\_sigmod\_2008.pdf:pdf},
isbn = {9781605581026},
issn = {07308078},
journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
keywords = {Distributed Data Stream Processing},
number = {SIGMOD 2008: Proceedings of the ACM SIGMOD International Conference on Management of Data 2008},
pages = {1123--1134},
title = {{SPADE: The system S declarative stream processing engine.}},
url = {https://access.korea.ac.kr/link.n2s?url=http://search.ebscohost.com/login.aspx?direct=true\&db=edselc\&AN=edselc.2-52.0-55649105542\&lang=ko\&site=eds-live\&scope=site},
year = {2008}
}
@article{Andrade2011,
abstract = {High-performance stream processing is critical in many sense-and-respond application domains—from environmental monitoring to algorithmic trading. In this paper, we focus on language and runtime support for improving the performance of sense-and-respond applications in processing data from high-rate live streams. The central tenets of this work are the programming model, the workload splitting mechanisms, the code generation framework, and the underlying System S middleware and Spade programming model. We demonstrate considerable scalability behavior coupled with low processing latency in a real-world financial trading application.},
author = {Andrade, H and Gedik, B and Wu, K.-L. and Yu, P S},
doi = {http://dx.doi.org/10.1016/j.jpdc.2010.08.007},
isbn = {0743-7315},
issn = {0743-7315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {Data stream processing,Scale-up strategies,Split/aggregate/join architectural pattern,Workload balancing},
number = {2},
pages = {145--156},
title = {{Processing high data rate streams in System S}},
url = {http://www.sciencedirect.com/science/article/pii/S074373151000167X},
volume = {71},
year = {2011}
}
@book{Few2006,
abstract = {Published January 2006, 223 pages. SUMMARY: Dashboards have become popular in recent years as uniquely powerful tools for communicating important information at a glance. Although dashboards are potentially powerful, this potential is rarely realized. The greatest display technology in the world won't solve this if you fail to use effective visual design. And if a dashboard fails to tell you precisely what you need to know in an instant, you'll never use it, even if it's filled with cute gauges, meters, and traffic lights. Don't let your investment in dashboard technology go to waste. This book will teach you the visual design skills you need to create dashboards that communicate clearly, rapidly, and compellingly. "Information Dashboard Design" will explain how to: Avoid the thirteen mistakes common to dashboard design Provide viewers with the information they need quickly and clearly Apply what we now know about visual perception to the visual presentation of information Minimize distractions, cliches, and unnecessary embellishments that create confusion Organize business information to support meaning and usability Create an aesthetically pleasing viewing experience Maintain consistency of design to provide accurate interpretation Optimize the power of dashboard technology by pairing it with visual effectiveness Stephen Few has over 20 years of experience as an IT innovator, consultant, and educator. As Principal of the consultancy Perceptual Edge, Stephen focuses on data visualization for analyzing and communicating quantitative business information. He provides consulting and training services, speaks frequently at conferences, and teaches in the MBA program at the University of California in Berkeley. He is also the author of "Show Me the Numbers: Designing Tables and Graphs to Enlighten." Visit his website at www.perceptualedge.com.},
author = {Few, Stephen},
booktitle = {The effective visual communication of data Sebastopol},
file = {:C$\backslash$:/Users/Jake/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Few - 2006 - Information Dashboard Design.pdf:pdf},
isbn = {0596100167},
pages = {223},
title = {{Information Dashboard Design}},
url = {http://proquest.safaribooksonline.com/0596100167?suggested=top},
year = {2006}
}
@article{Tufle1983,
abstract = {Tufte, E. R., \& Graves-Morris, P. R. (1983). The visual display of quantitative information (Vol. 2). Cheshire, CT: Graphics press.},
author = {Tufte, E},
isbn = {978-0961392147},
journal = {CT Graphics, Cheshire},
title = {{The Visual Display of Quantitative Information}},
url = {http://www.colorado.edu/UCB/AcademicAffairs/ArtsSciences/geography/foote/maps/assign/reading/TufteCoversheet.pdf},
year = {2001}
}
@misc{Amende2010,
keywords = {Information Visualization, Information Systems Suc},
title = {{A Structured Review of Information Visualization Success Measurement}},
url = {http://ewic.bcs.org/upload/pdf/ewic\_hci10\_paper1.pdf},
urldate = {2015-03-03}
}
@inproceedings{Wattenberg2008,
abstract = {We introduce the Word Tree, a new visualization and information-retrieval technique aimed at text documents. A word tree is a graphical version of the traditional "keyword-in-context" method, and enables rapid querying and exploration of bodies of text. In this paper we describe the design of the technique, along with some of the technical issues that arise in its implementation. In addition, we discuss the results of several months of public deployment of word trees on Many Eyes, which provides a window onto the ways in which users obtain value from the visualization.},
author = {Wattenberg, Martin and Vi\'{e}gas, Fernanda B.},
booktitle = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Case study,Concordance,Document visualization,Information retrieval,Many eyes,Search,Text visualization},
number = {6},
pages = {1221--1228},
pmid = {18988967},
title = {{The word tree, an interactive visual concordance}},
volume = {14},
year = {2008}
}
@book{Deleuze1987,
abstract = {The two of us wrote Anti-Oedipus together. Since each of us was several, there was already quite a crowd. Here we have made use of everything came within range, what was closest as well as farthest away. We assigned clever pseudonyms to prevent recognition. Why have we},
author = {Deleuze, Gilles and Guattari, PF and Guattari, Felix},
booktitle = {Writing},
chapter = {1},
number = {4},
pages = {151--169},
pmid = {12766771},
publisher = {University of Minnesota Press},
title = {{A thousand plateaus: Capitalism and schizophrenia}},
url = {http://www.jstor.org/stable/203963?origin=crossref$\backslash$nhttp://www.amazon.com/dp/0816614024$\backslash$nhttp://books.google.com/books?hl=en\&lr=\&id=C948Tsr72woC\&oi=fnd\&pg=PR9\&dq=a+thousand+plateaus:+capitalism+and+schizophrenia\&ots=9kIrnHqEGg\&sig=geid0WbkPt-qSY-Ub7WB7f\_eK5w},
volume = {19},
year = {1987}
}
@inproceedings{Olston2008,
abstract = {There is a growing need for ad-hoc analysis of extremely large data sets, especially at internet companies where inno- vation critically depends on being able to analyze terabytes of data collected every day. Parallel database products, e.g., Teradata, o er a solution, but are usually prohibitively ex- pensive at this scale. Besides, many of the people who ana- lyze this data are entrenched procedural programmers, who nd the declarative, SQL style to be unnatural. The success of the more procedural map-reduce programming model, and its associated scalable implementations on commodity hard- ware, is evidence of the above. However, the map-reduce paradigm is too low-level and rigid, and leads to a great deal of custom user code that is hard to maintain, and reuse. We describe a new language called Pig Latin that we have designed to t in a sweet spot between the declarative style of SQL, and the low-level, procedural style of map-reduce. The accompanying system, Pig, is fully implemented, and compiles Pig Latin into physical plans that are executed over Hadoop, an open-source, map-reduce implementation. We give a few examples of how engineers at Yahoo! are using Pig to dramatically reduce the time required for the develop- ment and execution of their data analysis tasks, compared to using Hadoop directly. We also report on a novel debugging environment that comes integrated with Pig, that can lead to even higher productivity gains. Pig is an open-source, Apache-incubator project, and available for general use.},
author = {Olston, Christopher and Reed, Benjamin and Srivastava, Utkarsh and Kumar, Ravi and Tomkins, Andrew},
booktitle = {Proceedings of the 2008 ACM SIGMOD international conference on Management of data - SIGMOD '08},
doi = {10.1145/1376616.1376726},
isbn = {9781605581026},
keywords = {H.2.3 Database Management: Languages},
pages = {1099},
title = {{Pig latin}},
url = {http://infolab.stanford.edu/~olston/publications/sigmod08.pdf$\backslash$nhttp://portal.acm.org/citation.cfm?doid=1376616.1376726},
year = {2008}
}
@inproceedings{Olston2008a,
abstract = {There is a growing need for ad-hoc analysis of extremely large data sets, especially at internet companies where inno- vation critically depends on being able to analyze terabytes of data collected every day. Parallel database products, e.g., Teradata, offer a solution, but are usually prohibitively ex- pensive at this scale. Besides, many of the people who ana- lyze this data are entrenched procedural programmers, who find the declarative, SQL style to be unnatural. The success of the more procedural map-reduce programming model, and its associated scalable implementations on commodity hard- ware, is evidence of the above. However, the map-reduce paradigm is too low-level and rigid, and leads to a great deal of custom user code that is hard to maintain, and reuse. We describe a new language called Pig Latin that we have designed to fit in a sweet spot between the declarative style of SQL, and the low-level, procedural style of map-reduce. The accompanying system, Pig, is fully implemented, and compiles Pig Latin into physical plans that are executed over Hadoop, an open-source, map-reduce implementation. We give a few examples of how engineers at Yahoo! are using Pig to dramatically reduce the time required for the develop- ment and execution of their data analysis tasks, compared to using Hadoop directly. We also report on a novel debugging environment that comes integrated with Pig, that can lead to even higher productivity gains. Pig is an open-source, Apache-incubator project, and available for general use.},
author = {Olston, Christopher and Reed, Benjamin and Srivastava, Utkarsh and Kumar, Ravi and Tomkins, Andrew},
booktitle = {Proceedings of the 2008 ACM SIGMOD international conference on Management of data - SIGMOD '08},
doi = {10.1145/1376616.1376726},
isbn = {978-1-60558-102-6},
issn = {07308078},
keywords = {dataflow language,pig latin},
pages = {1099},
title = {{Pig Latin: A Not-So-Foreign Language for Data Processing}},
year = {2008}
}
@inproceedings{Kunegis2013,
author = {Kunegis, J\'{e}r\^{o}me},
booktitle = {WWW 2013 Companion},
file = {:C$\backslash$:/Users/Jake/Documents/Thesis References/kunegis-koblenz-network-collection.pdf:pdf},
isbn = {9781450320382},
keywords = {network analysis,web observatory},
title = {{KONECT - The Koblenz Network Collection}},
year = {2013}
}
@misc{KONECT,
author = {KONECT},
title = {{Konect network dataset}},
url = {http://konect.uni-koblenz.de/}
}
@inproceedings{Didimo2011,
address = {Konstanz},
author = {Didimo, Walter and Liotta, Giuseppe and Romeao, Salvatore},
booktitle = {Graph Drawing 2010 Revised Selected Papers},
editor = {Brandes, Ulrik and Cornelsen, Sabine},
pages = {165--176},
publisher = {Springer Berlin Heidelberg},
title = {{Topology-Driven Force-Directed Algorithms}},
year = {2011}
}
@inproceedings{Islam2012,
abstract = {Hadoop is a massively scalable parallel computation platform capable of running hundreds of jobs concurrently, and many thousands of jobs per day. Managing all these computations demands for a workflow and scheduling system. In this paper, we identify four indispensable qualities that a Hadoop workflow management system must fulfill namely Scalability, Security, Multi-tenancy, and Operability. We find that conventional workflow management tools lack at least one of these qualities, and therefore present Apache Oozie, a workflow management system specialized for Hadoop. We discuss the architecture of Oozie, share our production experience over the last few years at Yahoo, and evaluate Oozie's scalability and performance.},
author = {Islam, Mohammad and Huang, Angelo K. and Battisha, Mohamed and Chiang, Michelle and Srinivasan, Santhosh and Peters, Craig and Neumann, Andreas and Abdelnur, Alejandro},
booktitle = {Proceedings of the 1st ACM SIGMOD Workshop on Scalable Workflow Execution Engines and Technologies - SWEET '12},
pages = {1--10},
publisher = {ACM Press},
title = {{Oozie: towards a scalable workflow management system for Hadoop}},
url = {http://dx.doi.org/10.1145/2443416.2443420},
year = {2012}
}
@book{Robbins2005,
address = {Hoboken},
author = {Robbins, Naomi},
pages = {49},
publisher = {John Wiley \& Sons},
title = {{Creating More Effective Graphs}},
year = {2005}
}
@article{Richthofer2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1404.6390v2},
author = {Richthofer, Stefan},
eprint = {arXiv:1404.6390v2},
file = {:C$\backslash$:/Users/Jake/Documents/Thesis References/JyNI\_Richthofer.pdf:pdf},
number = {Euroscipy},
pages = {59--64},
title = {{JyNI – Using native CPython-Extensions in Jython}},
year = {2013}
}
@misc{Ho2008,
author = {Ho, Ricky},
booktitle = {Pragmatic Programming Techniques},
title = {{Hadoop Map/Reduce Implementation}},
url = {http://horicky.blogspot.de/2008/11/hadoop-mapreduce-implementation.html},
urldate = {2015-07-12},
year = {2008}
}
@article{Gates2009,
abstract = {gnuplot plot},
author = {Gates, Alan F and Natkovich, Olga and Chopra, Shubham and Kamath, Pradeep and Narayanamurthy, Shravan M and Olston, Christopher and Benjaminn, Reed and Srinavasan, Santosh and Srivastava, Utkarsh},
doi = {10.1.1.151.3508},
file = {:C$\backslash$:/Users/Jake/Documents/Thesis References/pig\_experience.pdf:pdf},
isbn = {0000000000000},
issn = {2150-8097},
journal = {Vldb '09},
pages = {1--12},
title = {{Building a High-Level Dataflow System on top of Map-Reduce: The Pig Experience}},
url = {papers3://publication/uuid/08DB3582-AD5E-47BB-A370-589985B45F92},
year = {2009}
}
@inproceedings{Olston2008a,
abstract = {There is a growing need for ad-hoc analysis of extremely large data sets, especially at internet companies where inno- vation critically depends on being able to analyze terabytes of data collected every day. Parallel database products, e.g., Teradata, offer a solution, but are usually prohibitively ex- pensive at this scale. Besides, many of the people who ana- lyze this data are entrenched procedural programmers, who find the declarative, SQL style to be unnatural. The success of the more procedural map-reduce programming model, and its associated scalable implementations on commodity hard- ware, is evidence of the above. However, the map-reduce paradigm is too low-level and rigid, and leads to a great deal of custom user code that is hard to maintain, and reuse. We describe a new language called Pig Latin that we have designed to fit in a sweet spot between the declarative style of SQL, and the low-level, procedural style of map-reduce. The accompanying system, Pig, is fully implemented, and compiles Pig Latin into physical plans that are executed over Hadoop, an open-source, map-reduce implementation. We give a few examples of how engineers at Yahoo! are using Pig to dramatically reduce the time required for the develop- ment and execution of their data analysis tasks, compared to using Hadoop directly. We also report on a novel debugging environment that comes integrated with Pig, that can lead to even higher productivity gains. Pig is an open-source, Apache-incubator project, and available for general use.},
author = {Olston, Christopher and Reed, Benjamin and Srivastava, Utkarsh and Kumar, Ravi and Tomkins, Andrew},
booktitle = {Proceedings of the 2008 ACM SIGMOD international conference on Management of data - SIGMOD '08},
doi = {10.1145/1376616.1376726},
isbn = {978-1-60558-102-6},
issn = {07308078},
keywords = {dataflow language,pig latin},
pages = {1099},
title = {{Pig Latin: A Not-So-Foreign Language for Data Processing}},
year = {2008}
}
@article{Anscombe1973,
author = {Anscombe, F J},
doi = {10.1080/00031305.1973.10478966},
file = {:C$\backslash$:/Users/Jake/Documents/Thesis References/anscombe1973.pdf:pdf},
isbn = {00031305},
issn = {0003-1305},
journal = {The American Statistician},
number = {1},
pages = {17--21},
title = {{Graphs in Statistical Analysis}},
url = {http://www.sjsu.edu/faculty/gerstman/StatPrimer/anscombe1973.pdf},
volume = {27},
year = {1973}
}
@article{Shoresh2012,
author = {Shoresh, Noam and Wong, Bang},
file = {:C$\backslash$:/Users/Jake/Documents/Thesis References/nmeth.1829.pdf:pdf},
journal = {Nature Methods},
number = {1},
pages = {5},
title = {{Points of view: Data exploration}},
volume = {9},
year = {2012}
}
@inproceedings{Klasky2011,
abstract = {need for new I/O system for extreme-scale computing: 1)service oriented; 2)in situ processing!; 3) data staging;4) data management; 5) monitoring; 6) usability of I/O optimizations; 7) data formats},
author = {Klasky, Scott and Abbasi, Hasan and Logan, Jeremy and Parashar, Manish and Schwan, Karsten and Shoshani, Arie and Wolf, Matthew and Sean, Ahern and Altintas, Ilkay and Bethel, Wes and Luis, Chacon and Chang, CS and Chen, Jackie and Childs, Hank and Cummings, Julian and Docan, Ciprian and Eisenhauer, Greg and Ethier, Stephane and Grout, Ray and Lakshminarasimhan, Sriram and Lin, Zhihong and Liu, Qing and Ma, Xiaosong and Moreland, Kenneth and Pascucci, Valerio and Podhorszki, Norbert and Samatova, Nagiza and Schroeder, Will and Tchoua, Roselyne and Tian, Yuan and Vatsavai, Raju and Wu, John and Yu, Weikuan and Zheng, Fang},
booktitle = {SciDAC Conference},
file = {:C$\backslash$:/Users/Jake/Documents/Thesis References/scidac11-adios-insitu.pdf:pdf},
title = {{In Situ Data Processing for Extreme-Scale Computing}},
url = {http://pasl.eng.auburn.edu/pubs/scidac11-adios-insitu.pdf},
year = {2011}
}
@article{Battre2010,
abstract = {We present a parallel data processor centered around a programming model of so called Parallelization Contracts (PACTs) and the scalable parallel execution engine Nephele [18]. The PACT programming model is a generalization of the well-known map/reduce programming model, extending it with further second-order functions, aswellaswith Output Contracts that give guarantees about the behavior of a func- tion. We describe methods to transform a PACT program into a data flow for Nephele, which executes its sequential building blocks in parallel and deals with communication, synchronization and fault tolerance. Our definition of PACTs allows to apply several types of optimizations on the data flow during the transformation. Thesystemasawholeisdesignedtobeasgenericas(and compatible to)map/reduce systems, while overcoming several of their major weaknesses: 1) The functions map and reduce alone are not sufficient to express many data processing tasks both naturally and efficiently. 2) Map/reduce ties a program to a single fixed execution strategy, which is robust but highly suboptimal for many tasks. 3) Map/reduce makes no assumptions about the behavior of the functions. Hence, it offers only very limited optimization opportunities. With a set of examples and experiments, we illustrate how our system is able to naturally represent and efficiently execute several tasks that do not fit the map/reduce model well.},
author = {Battr\'{e}, Dominic and Ewen, Stephan and Hueske, Fabian and Kao, Odej},
doi = {http://doi.acm.org/10.1145/1807128.1807148},
file = {:C$\backslash$:/Users/Jake/Dropbox/Thesis/References/paper\_nephelePACTs\_2010.pdf:pdf},
isbn = {9781450300360},
journal = {ACM Symposium on Cloud Computing},
keywords = {cloud computing,map reduce,web-scale data},
pages = {119--130},
title = {{Nephele / PACTs : A Programming Model and Execution Framework for Web-Scale Analytical Processing Categories and Subject Descriptors}},
url = {http://doi.acm.org/10.1145/1807128.1807148$\backslash$nhttp://dl.acm.org/citation.cfm?id=1807148},
year = {2010}
}
@article{Amsterdamer2011,
abstract = {Workflow provenance typically assumes that each module is a "black-box", so that each output depends on all inputs (coarse-grained dependencies). Furthermore, it does not model the internal state of a module, which can change between repeated executions. In practice, however, an output may depend on only a small subset of the inputs (fine-grained dependencies) as well as on the internal state of the module. We present a novel provenance framework that marries database-style and workflow-style provenance, by using Pig Latin to expose the functionality of modules, thus capturing internal state and fine-grained dependencies. A critical ingredient in our solution is the use of a novel form of provenance graph that models module invocations and yields a compact representation of fine-grained workflow provenance. It also enables a number of novel graph transformation operations, allowing to choose the desired level of granularity in provenance querying (ZoomIn and ZoomOut), and supporting "what-if" workflow analytic queries. We implemented our approach in the Lipstick system and developed a benchmark in support of a systematic performance evaluation. Our results demonstrate the feasibility of tracking and querying fine-grained workflow provenance.},
archivePrefix = {arXiv},
arxivId = {1201.0231v1},
author = {Amsterdamer, Yael and Davidson, Susan B and Deutch, Daniel and Milo, Tova and Stoyanovich, Julia and Tannen, Val},
eprint = {1201.0231v1},
file = {:C$\backslash$:/Users/Jake/Downloads/p346-amsterdamer.pdf:pdf},
issn = {2150-8097},
journal = {Proceedings of the VLDB Endowment},
number = {4},
pages = {346--357},
title = {{Putting Lipstick on Pig : Enabling Database-style Workflow Provenance}},
url = {http://dl.acm.org/citation.cfm?id=2095693},
volume = {5},
year = {2011}
}
@article{Pauw2010,
abstract = {Stream processing is a new computing paradigm that enables con- tinuous and fast analysis of massive volumes of streaming data. Debugging streaming applications is not trivial, since they are typically distributed across multiple nodes and handle large amounts of data. Traditional debugging tech- niques like breakpoints often rely on a stop-the-world approach, which may be useful for debugging single node applications, but insufficient for streaming applications. We propose a new visual and analytic environment to support de- bugging, performance analysis, and troubleshooting for stream processing ap- plications. Our environment provides several visualization methods to study, characterize, and summarize the flow of tuples between stream processing op- erators. The user can interactively indicate points in the streaming application from where tuples will be traced and visualized as they flow through different operators, without stopping the application. To substantiate our discussion, we also discuss several of these features in the context of a financial engineering application.},
author = {Pauw, Wim De and Leţia, M and Gedik, B and Andrade, Henrique},
file = {:C$\backslash$:/Users/Jake/Downloads/2010 De Pauw.pdf:pdf},
journal = {Runtime Verification},
keywords = {debugging,performance,streaming applications,visualization},
pages = {18--35},
title = {{Visual debugging for stream processing applications}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-16612-9\_3},
year = {2010}
}
@article{Few2007,
author = {Few, Stephen},
file = {:C$\backslash$:/Users/Jake/Documents/Thesis References/save\_the\_pies\_for\_dessert.pdf:pdf},
journal = {Visual Business Intelligence Newsletter},
keywords = {Information Dashboard Design,Perceptual Edge,Pie chart,Show Me the Numbers,Stephen Few,Visual Business Intelligence Newsletter,graph design,pie charts,pop charts,table design},
pages = {1--14},
title = {{Save the Pies for Dessert}},
url = {http://www.perceptualedge.com/articles/visual\_business\_intelligence/save\_the\_pies\_for\_dessert.pdf},
year = {2007}
}
@article{Tufte1991,
abstract = {A remarkable range of examples for the idea of visual thinking, with beautifully printed pages. A real treat for all who reason and learn by means of images. - Rudolf Arnheim},
author = {Tufte, Edward},
doi = {10.1213/00000539-199103000-00040},
file = {:C$\backslash$:/Users/Jake/Documents/Thesis References/tufte, edward - envisioning information.pdf:pdf},
isbn = {0961392118},
issn = {0003-2999},
journal = {Bulletin of the Medical Library Association},
number = {3},
pages = {346--348},
pmid = {3895920},
title = {{Envisioning Information}},
volume = {79},
year = {1991}
}
@inproceedings{VanHam2009,
abstract = {We present a new technique, the phrase net, for generating visual overviews of unstructured text. A phrase net displays a graph whose nodes are words and whose edges indicate that two words are linked by a user-specified relation. These relations may be defined either at the syntactic or lexical level; different relations often produce very different perspectives on the same text. Taken together, these perspectives often provide an illuminating visual overview of the key concepts and relations in a document or set of documents.},
author = {{Van Ham}, Frank and Wattenberg, Martin and Vi\'{e}gas, Fernanda B.},
booktitle = {IEEE Transactions on Visualization and Computer Graphics},
doi = {10.1109/TVCG.2009.165},
file = {:C$\backslash$:/Users/Jake/Documents/Thesis References/phrase-net-rev5.pdf:pdf},
isbn = {1077-2626 VO - 15},
issn = {10772626},
keywords = {Text visualization,natural language processing,semantic net,tag cloud},
number = {6},
pages = {1169--1176},
pmid = {19834186},
title = {{Mapping text with phrase nets}},
volume = {15},
year = {2009}
}
@article{Wickham2006,
author = {Wickham, Hadley},
file = {:C$\backslash$:/Users/Jake/Documents/Thesis References/ggplot.pdf:pdf},
journal = {American Statistical Association 2006 Proceedings of the Section on Statistical Graphics},
keywords = {graphics; framework},
pages = {1--8},
title = {{An Implemetation of the Grammar of Graphics in \{R\}: ggplot}},
year = {2006}
}
@book{Wilkinson2005,
author = {Wilkinson, Leland},
edition = {2nd},
pages = {691},
publisher = {Springer},
title = {{The Grammar of Graphics}},
year = {2005}
}
@MISC{konect:2014:wikiconflict,
    title = {Wikipedia conflict network dataset -- {KONECT}},
    month = oct,
    year = {2014},
    url = {http://konect.uni-koblenz.de/networks/wikiconflict}
}

@article{konect:brandes09,
  author    = {Ulrik Brandes and
               J{\"u}rgen Lerner},
  title     = {Structural Similarity: Spectral Methods for Relaxed Blockmodeling},
  journal   = {J. Classification},
  volume    = {27},
  number    = {3},
  year      = {2010},
  pages     = {279-306},
}
@misc{ApacheSoftwareFoundation2014,
author = {{Apache Software Foundation}},
booktitle = {flink.apache.org},
title = {{Optimizer Plan Visualization Tool}},
url = {https://flink.apache.org/news/2014/01/26/optimizer\_plan\_visualization\_tool.html},
urldate = {7/18/2015},
year = {2014}
}
@inproceedings{Slingsby2009,
abstract = {We explore the effects of selecting alternative layouts in hierarchical displays that show multiple aspects of large multivariate datasets, including spatial and temporal characteristics. Hierarchical displays of this type condition a dataset by multiple discrete variable values, creating nested graphical summaries of the resulting subsets in which size, shape and colour can be used to show subset properties. These 'small multiples' are ordered by the conditioning variable values and are laid out hierarchically using dimensional stacking. Crucially, we consider the use of different layouts at different hierarchical levels, so that the coordinates of the plane can be used more effectively to draw attention to trends and anomalies in the data. We argue that these layouts should be informed by the type of conditioning variable and by the research question being explored. We focus on space-filling rectangular layouts that provide data-dense and rich overviews of data to address research questions posed in our exploratory analysis of spatial and temporal aspects of property sales in London. We develop a notation ('HiVE') that describes visualisation and layout states and provides reconfiguration operators, demonstrate its use for reconfiguring layouts to pursue research questions and provide guidelines for this process. We demonstrate how layouts can be related through animated transitions to reduce the cognitive load associated with their reconfiguration whilst supporting the exploratory process.},
author = {Slingsby, Aidan and Dykes, Jason and Wood, Jo},
booktitle = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Geovisualization,exploratory,guidelines,hierarchical,layout,notation},
number = {6},
pages = {977--984},
title = {{Configuring hierarchical layouts to address research questions}},
volume = {15},
year = {2009}
}
@inproceedings{Schreck2006,
abstract = {Hierarchical relationships play an utmost important role in many application$\backslash$ndomains. The appropriate visualization of hierarchically structured$\backslash$ndata sets can contribute towards supporting the data analyst in effectively$\backslash$nanalyzing hierarchic structures using visualization as a user friendly$\backslash$nmeans to communicate information. Information Visualization has contributed$\backslash$na number of useful techniques for visualization of hierarchically$\backslash$nstructured data sets. Yet, the support for certain regularity requirements$\backslash$nas arising from many data element types has to be improved. In this$\backslash$npaper, we analyze an existing variant of the popular TreeMap family$\backslash$nof hierarchical layout algorithms, and we introduce a novel TreeMap$\backslash$nalgorithm supporting space efficient layout of hierarchical data$\backslash$nsets providing global regular layouts. We detail our algorithm, and$\backslash$nwe present applications on a real-world data set as well as experiments$\backslash$nperformed on a synthetic data set, showing its applicability and$\backslash$nusefulness.},
author = {Schreck, Tobias and Keim, DA and Mansmann, F},
booktitle = {Spring Conference on Computer Graphics},
keywords = {2006,acm siggraph,april 20-22,casta papiernicka,data,graphics,in,regular treemap layouts for,sccg,slovak republic,spring conference on computer,st publ,visual analysis of hierarchical},
title = {{Regular TreeMap Layouts for Visual Analysis of Hierarchical Data}},
url = {http://www.gris.informatik.tu-darmstadt.de/~tschreck/docs/sccg06.pdf},
year = {2006}
}
@inproceedings{Wood2008,
abstract = {Existing treemap layout algorithms suffer to some extent from poor or inconsistent mappings between data order and visual ordering in their representation, reducing their cognitive plausibility. While attempts have been made to quantify this mismatch, and algorithms proposed to minimize inconsistency, solutions provided tend to concentrate on one-dimensional ordering. We propose extensions to the existing squarified layout algorithm that exploit the two-dimensional arrangement of treemap nodes more effectively. Our proposed spatial squarified layout algorithm provides a more consistent arrangement of nodes while maintaining low aspect ratios. It is suitable for the arrangement of data with a geographic component and can be used to create tessellated cartograms for geovisualization. Locational consistency is measured and visualized and a number of layout algorithms are compared. CIELab color space and displacement vector overlays are used to assess and emphasize the spatial layout of treemap nodes. A case study involving locations of tagged photographs in the Flickr database is described.},
author = {Wood, Jo and Dykes, Jason},
booktitle = {IEEE Transactions on Visualization and Computer Graphics},
doi = {10.1109/TVCG.2008.165},
file = {:C$\backslash$:/Users/Jake/Documents/Thesis References/wood\_spatially\_2008.pdf:pdf},
isbn = {1077-2626 VO - 14},
issn = {10772626},
keywords = {CIELab,Cartograms,Geographic information,Geovisualization,Tree structures,Treemaps},
number = {6},
pages = {1348--1355},
pmid = {18988983},
title = {{Spatially ordered treemaps}},
volume = {14},
year = {2008}
}
@article{Lin2010,
abstract = {Graphs are analyzed in many important contexts, includ- ing ranking search results based on the hyperlink struc- ture of the world wide web, module detection of protein- protein interaction networks, and privacy analysis of social networks. Many graphs of interest are dicult to analyze because of their large size, often spanning millions of vertices and billions of edges. As such, researchers have increasingly turned to distributed solutions. In particular, MapReduce has emerged as an enabling technology for large-scale graph processing. However, existing best practices for MapReduce graph algorithms have signi cant shortcomings that limit performance, especially with respect to partitioning, serial- izing, and distributing the graph. In this paper, we present three design patterns that address these issues and can be used to accelerate a large class of graph algorithms based on message passing, exempli ed by PageRank. Experiments show that the application of our design patterns reduces the running time of PageRank on a web graph with 1.4 billion edges by 69\%},
author = {Lin, Jimmy and Schatz, Michael},
doi = {10.1145/1830252.1830263},
file = {:C$\backslash$:/Users/Jake/Documents/Thesis References/graphAlgo in mapReduce paper p78-lin.pdf:pdf},
isbn = {9781450302142},
journal = {Mlg},
pages = {78--85},
title = {{Design Patterns for Efficient Graph Algorithms in MapReduce}},
year = {2010}
}
@article{Bederson2002,
abstract = {Treemaps, a space-filling method of visualizing large hierarchical data sets, are receiving increasing attention. Several algorithms have been proposed to create more useful displays by controlling the aspect ratios of the rectangles that make up a treemap. While these algorithms do improve visibility of small items in a single layout, they introduce instability over time in the display of dynamically changing data, fail to preserve order of the underlying data, and create layouts that are difficult to visually search. In addition, continuous treemap algorithms are not suitable for displaying quantum-sized objects within them, such as images. This paper introduces several new treemap algorithms, which address these shortcomings. In addition, we show a new application of these treemaps, using them to present groups of images. The ordered treemap algorithms ensure that items near each other in the given order will be near each other in the treemap layout. Using experimental evidence from Monte Carlo trials, we show that compared to other layout algorithms ordered treemaps are more stable while maintaining relatively favorable aspect ratios of the constituent rectangles. A second test set uses stock market data. The quantum treemap algorithms modify the layout of the continuous treemap algorithms to generate rectangles that are integral multiples of an input object size. The quantum treemap algorithm has been applied to PhotoMesa, an application that supports browsing of large numbers of images.},
author = {Bederson, Benjamin B. and Shneiderman, Ben and Wattenberg, Martin},
doi = {10.1145/571647.571649},
file = {:C$\backslash$:/Users/Jake/Documents/Thesis References/treemaps.pdf:pdf},
isbn = {0730-0301},
issn = {07300301},
journal = {ACM Transactions on Graphics},
number = {4},
pages = {833--854},
title = {{Ordered and quantum treemaps: Making effective use of 2D space to display hierarchies}},
volume = {21},
year = {2002}
}
@article{PlaisantCFekete2008,
author = {{Plaisant C Fekete}, J D Grinstein G},
file = {:C$\backslash$:/Users/Jake/Documents/Thesis References/InfoViz\_Contest.pdf:pdf},
journal = {IEEE transaction on visualization and computer graphics},
number = {May},
pages = {120--134},
title = {{Promoting Insight Based Evaluation of Visualization: From Contest to Benchmark Repository}},
volume = {14},
year = {2008}
}
@misc{Shneiderman1992,
abstract = {this paper deals with a two-dimensional (2-d) space-filling approach in which each node is a rectangle whose area is proportional to some attribute such as node size. Research on relationships between 2-d images and their representation in tree structures has focussed on node and link representations of 2-d images. This work includes quad-trees (Samet, 1989) and their variants which are important in image processing. The goal of quad trees is to provide a tree representation for storage compression and efficient operations on bit-mapped images. XY-trees (Nagy \& Seth, 1984) are a traditional tree representation of two-dimensional layouts found in newspaper, magazine, or book pages. Related concepts include k-d trees (Bentley and Freidman, 1979), which are often explained with the help of a},
author = {Shneiderman, Ben},
booktitle = {ACM Transactions on Graphics},
number = {1},
pages = {92--99},
title = {{Tree visualization with tree-maps: 2-d space-filling approach}},
volume = {11},
year = {1992}
}
@book{Fry2008,
author = {Fry, Ben},
file = {:C$\backslash$:/Users/Jake/Documents/Thesis References/visualizing\_data.pdf:pdf},
isbn = {978-0-596-51455-6},
pages = {384},
title = {{Visualizing data}},
url = {http://dl.acm.org/citation.cfm?id=529269},
year = {2008}
}

