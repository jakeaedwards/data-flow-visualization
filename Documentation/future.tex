\chapter{Future Work}
\label{sec:futurework}

\section{Visualizations}
\label{sec:future_viz}
\INITIAL{T}{he visualizations used so far} in this work demonstrate an adequate base level of functionality for achieving the desired results in most in-situ analysis scenarios. The obvious area for improvement here is in expanding the capabilities of the system to handle specific graphical tasks without the ad-hoc design of graphical classes. Because so much work has been done in recent decades in the field of formalizing visualization practices, some works have emerged which have had serious influences on most well-used libraries built for this task. One such work is "The Grammar of Graphics" by Leland Wilkinson \cite{Wilkinson2005}. 

\paragraph{Grammar of Graphics}
The Grammar of Graphics is a seminal work in the field of scientific visualization which defines a rigorous method for developing graphics based on the data and scenario presented.  This gives rigid reasoning behind the application of most common graphics in a scientific environment, and more tangibly has resulted in the creation of several graphics libraries which are capable of building any visualization described by the grammar. The book deals exclusively with static graphics as would be seen in a statistical or scientific analysis vs. some kind of interactive business visualization.

\paragraph{Libraries}
Though there have been several projects and software libraries based on the theoretical foundations laid out in The Grammar of Graphics since its publication, most are written in R rather than languages which are more commonly supported through the APIs of data flow based analysis platforms. The reasons for this are complex, but generally speaking major factors include the widespread use of R for analysis and the complexity of implementation in other development environments such as java. Typically, when scientific visualizations are needed analysts tend to turn to other languages in which a solution does exist.

\paragraph{R \& ggplot}
The statistical programming language R was purpose built with data analysis in mind. As such, it has a large user base across all scientific disciplines and has become a de facto standard for statistical computing and visualization. One of the most widely used libraries for visualization in R, is "ggplot" \cite{Wickham2006} which is based on "The Grammar of Graphics" and was first released only a year after the publication of the book. Ultimately, there is a strong argument for this library within R being the most convenient way of visualizing data by virtue of existing user base, documentation, and simple syntax alone. The difficulty of course is integrating R into the existing code, though several approaches are available. 

\paragraph{R in Java}
There has been work in both directions of Java/R interoperability, with some R users desiring the performance or flexibility of specific java methods or libraries, and java users desiring the analytical tools provided by R. From R, the RJava project allows users to call java functions, and likewise the RCaller package allows java users to execute R code by making calls to a local script. The problem with such a method in the case of in-situ analytics lies in a reliance on the execution environment being configured for R. Additionally, with a library such as RCaller there can be issues when executing in a cluster environment, as data transit has not been optimized for distribution beyond four nodes. As currently implemented this is not a problem, but reliance on such a library would create hurdles when developing real-time functionality. A better solution is a project such as Renjin, which is a version of the R language written entirely in java, so that there are no requirements placed on the execution environment. For general purpose analytical R usage this is an elegant solution, but for visualization we rely on packages more than the R language itself, and the bulk of commonly used packages are written in C and have not yet been themselves rewritten to run under a JVM based environment such as a Renjin application.

\paragraph{Python}
Python, though newer than R is quickly becoming as popular in the realm of data science and statistical computing in general. Though it doesn't yet have the same array of scientific computing libraries as R, there are several ongoing projects which aim to close the gap. Of course high on the list of items to be developed for python was a version of ggplot. This work has been largely completed, save for minor changes which are still being made based on user feedback and some small aesthetic adjustments which are ongoing. Like in R, there are projects which aim to allow for the execution of python code in java and vice versa. On particularly popular option among such projects is Jython, which like Renjin is an implementation of the python language written entirely in java. Unfortunately, the exact same pitfall is encountered here, with the modules required to utilize the ggplot python library being built using C, and thus not usable in a JVM only environment. 

\paragraph{JyNi}
As opposed to the case with Renjin however, a compatibility layer has been developed for Jython with the express purpose of allowing the use of C based scientific python libraries in java applications. This compatibility layer is called "JyNi" \cite{Richthofer2013} and is still actively in development. Unfortunately, this active development means that as of the time of this writing, ggplot is one of several libraries which still presents issues even with the compatibility layer. This issues is expected to be resolved by the end of 2015, and thus will soon be a viable option for accessing a very robust visualization library.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Real-Time Results}
\label{sec:realtime}
\INITIAL{D}{istribution in analysis systems} following the general data flow graph model all operate very similarly in concept. This means that generally speaking, we can expect elements of our data set to be partitioned across a cluster in a uniformly distributed way. Because we may want to examine the intermediate data set at a point when it has not yet been centralized in one location, we must collect it piecemeal from each node in the cluster. This could be achieved by sending the data sets from each node in the cluster to the visualizer for summary. Real-time results would be particularly useful in stream processing applications, where the values being processed are expected to change indefinitely during execution.

\paragraph{Message Passing}
Message passing could allow us to invoke a send message call from each in-situ data collector operating on a shard of the complete data set, and then receive it in the visualizer. The visualizer can perform whichever operations are needed in order to merge the data sets considering the original locations and timing in order to generate useful output. Many simple frameworks for message passing such as RabbitMQ are being used with major Apache projects and would be simple to include within this work without introducing additional dependencies or platform requirements. Additionally, as briefly mentioned in Chapter 4, some simple design patterns already exist for message passing in Map-Reduce jobs \cite{Lin2010}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Interface}
\label{sec:interface}
\INITIAL{T}{hough it does not present a research problem}, a major improvement to usability of this visualization method would be the inclusion of some kind of interactive interface. Because each of the visualized and collected datasets are indexed within the visualizer, they can be kept for offline access and organized for manipulation or organization by users. Such a user interface could appear in a similar fashion to the web interface provided for visualizing the flink execution plan. It would be simple to add interactivity and user input, which could potentially even be expanded to allow for the deeper analysis provided by som of the discussed methods such as phrase nets and word trees.

\paragraph{Javascript}
While there are many limitations to the available visualization tools and libraries in Java, visualization is a much more common task in web based applications and thus visualization libraries built for the web are much more robust in general. Because the visualization classes in this work are mainly used for data formatting and are not particularly coupled to Java, the change to using a robust javascript library such as D3 over the processing tools used here would be simple.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Automation}
\label{sec:automation}
\INITIAL{C}{urrently, code modification is required} for the data collector to be able to identify when to collect, and what format of data to expect. Additionally, the developer of an analysis program is expected to specify which type of visualization(s) to generate for each of the collected data sets. With the addition of some pattern matching, it could be possible to perform these actions without the user needing to add any code to an analysis program.

\paragraph{Automatic Collection}
Identifying points at which to collect data is a very simple problem. Any of the points of interest for collection will occur either immediately after a data set is populated or modified, or directly before the end of the program when the data is in its final state. In terms of the program execution plan, if each node in the generated data flow graph represents an transform or data source/destination, we can simply collect data at each edge. Complications could potentially arise in cases where control flow logic such as looping would generate an impractically large number of collected data sets with limited or useless semantic differences between one another; however, these situations could be mitigated by simple ignoring potential collection points from within complex control structures.

\paragraph{Automatic Visualization}
Given the rigid nature in which the application of visualizations  are often described, particularly in the case of the Grammar of Graphics \cite{Wilkinson2005}, it is feasible that a method of determining the most appropriate visualization for a data set would be programmatically. The first major problem here would be in handling the relatively large number of potential input formats in which data may present itself in an analysis program and detecting mismatches or errors therein. In this work, much of the complication is avoided by virtue of the developer-written calls to the collector, in which they are relied upon to provide accurate class information. Error detection and handling would have to be very robust in such a scenario, so that simple errors could be visualized for the user rather than simply causing failure in the visualization process. Once features such as type, dimensionality, and size of data have been detected through some mechanism, some conditional logic could be applied which would result in the program selecting the most appropriate general purpose visualization for the provided data set.  