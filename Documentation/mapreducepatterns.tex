\chapter{MapReduce Patterns}
\label{sec:mapreducepatterns}
\INITIAL{T}{here are many scenarios} in which MapReduce can be applied. Because this work is meant to applicable to any MapReduce job, example cases must be selected in order to cover a varied range of analyses and data types. In this case, the analyses chosen attempt to cover the major MapReduce pattern categories as presented in the text "MapReduce Design Patterns" \cite{Miner2012}. Of course, in addition to the analysis itself the type of data being visualized is key. As such, these design patterns have been chosen in order to represent the most essential functions of MapReduce analysis, and thus the widest applicable range of input data sets.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summarization Patterns}
\label{sec:summarization}
\INITIAL{N}{umerical aggregation tasks} across groupings in a data set are the most common tasks which are encountered in MapReduce analysis programs. This base grouping of data is of course one of the most core functions of the MapReduce paradigm, and thus is often the most straightforward and commonly encountered type of analysis. Because of the simplicity of summarization tasks, they are frequently the first form of analysis performed in the exploration of a new data set. This makes summarization patterns a crucial, albeit straightforward, point of focus for any work concerning the evaluation of unknown factors in data.

\paragraph{Numerical Summary}
Numerical summarization is a general pattern for calculation aggregate statistical values across groupings in a data set. Most data sets will be too large for a human to be able to extract meaningful patterns from viewing individual records. Hence, when we are dealing with data that can be grouped by fields in a semantically meaningful way and can be sensibly aggregated or counted we can summarize in hopes of revealing insight. This pattern is analagous to performing an aggregation after a group by expression in a SQL-like context.

\paragraph{Inverted Index}
Inverted indexes are often constructed in scenarios when it is useful to associate some key term with a set of related results within a dataset. This serves to improve search speed by eliminating the need to examine each possible result in a large data set. It does this by pre-restricting the potential results to those which are known to be associated with the search term provided. This differs structurally from the numerical summarization pattern in that the result will be a set of record identifiers mapped onto some search keyword, rather than the relatively simpler group identifier-statistic pairs provided by numerical summarization. Though the actual implementation of the analysis differs, the information which would be useful for basic analysis remains very similar. Namely, the most crucial information here would be the number and nature of results associated with each keyword. This is generally identical to the previous visualization scenario, if we consider the dataset to be visualized as a set of keywords and associated statistical result set metadata (Number of results, average result size, etc.).  

\paragraph{Counting}
Semantically, counting problems could be considered a subset of numerical summarization. In scenarios where we require only a simple count or summary of a specific field, we could output the key of a record with a count of one and then reduce to generate a final count. The counting pattern instead utilizes the internal counting mechanism of our mapreduce framework to render the use of a reduce or summation stage unnecessary. One can simply create a counter with the ID of the field to be counted and increment by one until logging the result before the end of the execution. An example of a case where this is more efficient than a normal numerical summarization is the classic word count example. As the differences between this pattern differ in implementation rather than goal, the visualization scenarios are likewise identical to those found with numerical summarization. 

\section{Filtering Patterns}
\label{sec:filtering}

\INITIAL{F}{iltering patterns} are primarily concerned with understanding a specific part of an overall data set. As such, all filtering patterns are defined by the fact that they do not alter records in the data set, and that they each return a subset of the original data set. This can be considered analogous to search tasks, in that a set of relevant records is returned based on some provided criteria. All filtering tasks require that data be organized into discrete records.

\paragraph{Sampling}
In the context of map-reduce problems in particular, filtering is very useful for sampling. In situations where the data set to be analyzed is too large for processing in full, sampling methods can provide representative subsets. In some cases there are analysis based reasons to perform sampling; such as separating data into training and testing sets for machine learning algorithms. While this alone makes filtering an interesting use case, sampling is of specific interest for this work due to it's frequent application in exploratory analyses. When testing of an analysis job is performed on a large unknown data set, it is intuitive to simply select an arbitrary subset of records to analyze for debugging purposes. This likely provides a skewed view of the data, and an appropriately sampled data set will provide a more representative view of the task at hand. 

\paragraph{Filtering}
Filtering itself serves as an abstract pattern for the many different types of filtering that can occur in an analysis job. This is of course the most basic filtering function, wherein a subset of the records in a data set are removed based on whether they are of interest or not. In processing systems such as Flink, the purpose is very typically to collect a large sum of data in one place. Simple filtering can serve to either pare some unnecessary data from this sum, or focus on a small set of records or attributes which are particularly important.

\paragraph{Bloom Filtering}
Bloom filtering performs much the same task as basic filtering, but with added control in the method through which records are selected for filtering. When applying a bloom filter we extract a feature from each record, and compare that value to a set of values represented by the filter. The primary difference between this and standard filtering are that the decision to filter a given record is determined by the result of a set operation against our filter values. For this approach to be relevant, we must have extractable features which can be compared to the set of filter values, and these filter values must be predefined. It is possible when applying a bloom filter that some records will not be filtered out when they should have been, so they should only be used in scenarios where false positives are acceptable. Such a scenario could occur when prefiltering before performing a more thorough, and much more expensive, robust filtering.

\paragraph{Top N}
Performing a top N filter on a data set is of course distinct in that the size of the output data set is known before filtering occurs. Functionally, this is of course very similar to the previous two filtering methods. The application however differs in that there is a clear semantic application of this filter, the collection of outliers. In map-reduce settings this can be a particularly interesting problem as the typical method for accomplishing such a task in another context generally involves sorting the items in a data set, an extremely involved task using MapReduce. This provides additional information about our output, as we can infer that the output of a top N filter will be significantly smaller than the original data set; otherwise a total ordering is often a more suitable approach.

\paragraph{Distinct}
Filtering for distinct records is of course self-explanatory in meaning. There are several applications for such a filter, the most common of which is most likely removing duplicate records. In collecting data sets, duplication of records is a frequent data quality issue which can both add unnecessary processing time and skew analysis results. 


\section{Data Organization Patterns}
\label{sec:dataorganization}
\INITIAL{D}{ata organization} problems can present themselves in many ways, and have a wide variety of motivations behind them. With respect to big data problems in particular, the way that data is partitioned, sharded, or sorted can have serious implications for performance. If we consider in-situ processing in particular, there are many cases where data will need to be restructured for further analysis beyond that which is performed in the map-reduce context.

\paragraph{Structured to Hierarchical}
The structured to hierarchical pattern takes a row based data set and transforms it into a hierarchical format such as JSON or XML. Because MapReduce systems don't care about data format, hierarchical data structures can be immensely helpful in avoiding joins.

\paragraph{Partitioning}
Partitioning of course separates data into categories. This can be considered semantically similar to a summarization task without any form of aggregation, although the implementation may differ significantly. The major requirement of a partitioning job is to know in advance how many partitions should be created. This can be user provided, or derived from a prior analysis job, in which case the number of partitions may remain unknown to the user. Partitioning becomes very interesting for performance reasons when the partitions are actually sharded across different physical machines in a cluster.

\paragraph{Binning}
Binning can often be used to solve the same problems as partitioning and is very similar overall. The key difference between the two lies in implementation; binning splits data in the map phase instead of within a partitioner, eliminating the need for a reduce phase. The data structures, and therefore types of visualizations that we would want to see in such a scenario, are identical.

\paragraph{Sorting}
The total order sorting pattern is of course concerned with the order of records within a data set. While sorting is a fairly standard operation in the realm of sequential programming, it is  very expensive and thus less applied in general in MapReduce environments. Achieving total order sorting on data in parallel requires that nor only the data in each node of the cluster is sorted, but that we can logically join these individual segments of data to form a cohesive sorted whole upon output from the analysis task. The first step in this process is to establish ranges of values within the data set which can be expected to partition evenly across the nodes in the cluster provided. From this point, the data can be partitioned based on the established ranges, and we can sort each individually. The output files from each of these partitions should be appropriately named so that their order within the ranges of values possible is known, then the files can be merged and all output values are in a total order. The cost of this pattern stems from the fact that the data must be loaded and operated on twice; once to determine value ranges for partitioning and then again for the actual sort operation. 

\paragraph{Shuffling}
Providing essentially the opposite function of the sorting pattern is the Shuffling pattern. This operation serves to randomize the order of rows within a dataset, as well as the order of any attribute values which may appear therein. Applications for such a task are most commonly seen in anonymizing sensitive data sets or performing random sampling tasks. The performance of a shuffling operation is much less expensive than a sort as there is no requirements on the organization of which piece of data is sent to which partition. This also ensures that we will see balanced sizes across all partitions and output files. Aside from these benefits, the shuffling pattern nonetheless still requires that the entire data set be sent over the network and will benefit from the  use of many reducers.

\section{Join Patterns}
\label{sec:joins}
\INITIAL{I}{t is relatively uncommon} for all of the data used in a large analysis to stem from the same source. Data can originate from log files, databases, or from a sensor stream feeding directly into HDFS. While joins are simple to perform in other development environments, as is the case with SQL, often requiring only one simple command, in MapReduce environments much of the work must be performed by the developer. Because of the inherent complexity of join operations, there are several useful patterns for implementing them in MapReduce depending on what the specific needs of the scenario are. 

\paragraph{Reduce Side Join}
The simplest of the core join patterns is the reduce side join. It can be used to execute any of the basic joins seen in a standard SQL implementation (inner, left outer, right outer, full outer, antijoin, and cartesian product) and sets no limits on the sizes of data sets involved. The general use case for such a join pattern is a scenario where flexibility is desired, and a foreign key exists upon which to perform the join. In implementation terms, a mapper extracts the foreign key from each record and outputs a pair with the foreign key as a key and the entire record as a value. Then, a reducer creates temporary lists for each foreign key value across all data sets, which are then combined based on the desired join logic. This is also the most expensive of the standard join patterns because the foreign key output from the map operation means that no pre-filtering can occur. This cost can be somewhat reduced by applying a bloom filter to the records being output from the mapper. However, with such a filter the reduction in network I/O will be more useful in the case of an inner join than it will with a full outer join or antijoin; which both require all output to be sent to the reducer. 

\paragraph{Replicated Join}
In cases where only one of the data sets to be joined is large, a replicated join can be applied. In this scenario all data sets excluding the large one are read into memory, thus eliminating the need for a reduce step. The join can be performed entirely in the map phase, with the large dataset acting as input. This is of course a very strict limit set on the size of the small datasets, which is detetrmined by the size of the JVM heap. Additionally, this is really only a valid approach for an inner or left outer join where the large dataset can act as the left data set in the join. 

\paragraph{Composite Join}
The reduce phase of a join can also be eliminated for larger datasets, through the use of the composite join pattern. This method is limited however by the requirement that the datasets be organized in a specific way. Specifically, all data sets must be able to be read with the foreign key as input to a mapper, they must all have the same number of partitions, and they must be ordered by the foreign key. This is very useful in cases where inner or full outer joins are desired on structured data sources, but in cases such as in-situ processing where guarantees on features of the data set are unknown this is not a practical option for implementation.

\paragraph{Cartesian Product}
The last resort in terms of performance is of course a cartesian product. Execution a join by cartesian product is not very well suited to MapReduce, as the operation cannot be parallelized very well and requires more computation time and network traffic than another join. Nonetheless, there are occasions when there is no other option and it must be performed. The most likely candidates for such a join are text document or media analysis where discrete record fields which can be identified as foreign keys are not easily extracted. 

\paragraph{Visualizing Joins}
While joins undoubtedly perform a vital task in an analysis flow, they do not directly present fertile ground for building visualizations. In the context of performing an in-situ analysis in particular, where we are interested in detecting problems with the data  being joined for debugging purposes for example, most problems will be found easily using other methods. For example, an expected issue with joining data would be some kind of mismatch with the keys being used or an unexpected number of results in the joined set. In the former case, an unexpected value in a key column or a complete mismatch will produce an error in the analysis task code which will not require any special technique to detect. In the latter case, where the join succeeds but some records are unexpectedly lost or included in the join, we will require the application of a pattern from one of the previous categories. Most likely this will be a simple matter of counting records from each set being joined, perhaps by some attribute set, and then visualizing this result similarly to any other encountered in a summarization problem. So, while the task itself is quite different, we can consider the associated visualization requirements to be subsumed by those of the previous categories in all normal situations.

\section{Meta Patterns}
\label{sec:metapatterns}
\INITIAL{M}{eta patterns} encompass patterns which deal with the handling of smaller patterns rather than solving particular problems themselves. Because they don't focus on particular problems they don't yield much interesting information for visualization in and of themselves. However, they do provide insight into the way that a large analysis job might be constructed using the previously discussed patterns. This in turn demonstrates the scalability of the previously discussed patterns and by extension the scalability of visualization solutions applied to them individually.

\paragraph{Job Chaining}
Perhaps the most intuitive of the meta patterns in MapReduce is job chaining. Large problems are often not easily solved with a single MapReduce job, and thus require a series of jobs to be chained together somehow. In the simplest case, this could mean that several jobs are executed in parallel while others have their input provided by previously completed jobs. This is generally a process which relies heavily on developers, as MapReduce systems are often not equipped to handle more than one job very well and a certain degree of manual coding is required. There are some tools which are being developed to handle this issue, such as Apache's Oozie \cite{Islam2012}. Without such tools there are still several options for developers to handle such issues, such as creating  a job driver. This is very straightforward, in that a developer simply creates a generic driver task which will call the drivers for sub-tasks in turn when appropriate. Perhaps the most difficult part of such an approach is determining what the most appropriate ordering for execution is and which jobs will require input from some parent job. This approach can also be applied externally by using some kind of script to execute jobs rather than a driver class in the analysis environment itself. The JobControl and ControlledJob classes form a system for chaining MapReduce jobs in Hadoop, but for simpler applications this may be unnecessarily complicated. 

\paragraph{Chain Folding}
Chain folding provides a method through which job chains can be optimized further. Because each record can be submitted to multiple mappers and map phases are completely shared-nothing we know that each record will be assessed on its own regardless of grouping or data organization. This means that we can take the map setps of multiple jobs and combine them into a single map phase, significantly reducing I/O load stemming from data movement through the MapReduce pipeline.  

\paragraph{Job Merging}
Another method which is focused on reducing the I/O costs incurred by jobs is job merging. Job merging applies when more than one job uses the same set(s) of data during their execution. In some cases, if the data set is large enough the initial loading stage may even be the most costly portion of the analysis flow, and is divided for each job that can be merged. There are many complications with merging jobs, not the least of which are the requirements that all keys used in intermediate stages and output formats must match between jobs so that they are both operating on the same data types. A single map function can then be used to perform the tasks of map functions from both of the sub-jobs, adding a tag to output records to identify which mapper task it is associated with. Reducers can then use conditional logic to decide what kind of reduce task to perform based on the tag provided during mapper output. The reduce results can be split to separate destinations at this point for distinct processing on a presumably much smaller set of records.

\section{Summary}
\label{sec:pattern_summary}
\INITIAL{E}{ach of the previously} presented patterns provides a solution to some of the most commonly encountered problems in a data flow graph based analysis scenario. After examining each it quickly becomes clear that while many are directly analysis based, such as counting or filtering, we also see many patterns which are much less reliant (and have less affect) on the visualizable features of the underlying data. These patterns are, by definition, solutions to some of the most commonly encountered tasks in MapReduce analysis. Demonstrating that visualizations are either generally not useful in each case or are attainable using the solution presented in this work serves to demonstrate that this work can be applied to the most commonly encountered analysis tasks and provide the desired outcomes. 

\paragraph{Use Cases}
Table \ref{tbl:usages} shows a visual summary of the core visualizations discussed in the previous chapter as matched with the Map Reduce pattern types with which they are likely to be applied. Numerical summary and counting of course both imply category comparisons, which indicate bar charts and line charts being applied depending on expected trends in the data set or a constant dimension such as time. Inverted indexes as a summarization pattern is of course excluded, because this is not a task for which visual analysis of the data would be relevant. Filtering of course can also be applied to categorical data sets, but also provides a more likely application scenario for scatter plots. When filtering data points, visualizing to determine if those records that have been filtered out remove a cluster or reduce noise on a scatter plot is a very common visualization task. Data organization and join patterns are not particularly well suited for analysis using standard visualizations, for different reasons. Visualizing data which has been structured through an organization pattern is heavily dependent on the structure of the outgoing data (XML, JSON, etc.) and will rely on some custom logic and processing in almost all cases. Joins suffer the opposite problem, where the basic exploratory information one would want to extract from a join operation itself is almost always limited to cardinality, and it is probably most efficient to simply output a summary statistic rather than render any kind of visualization. Meta patterns of course relate to program flow, and as such encompass any of the previous pattern types and their related visualizations. 

\paragraph{Other Patterns}
Other design patterns in the realm of Map-Reduce problems have been introduced, but generally these are related to specific problem domains such as graph processing algorithms or tasks such as message passing and optimization \cite{Lin2010}. Though these patterns are useful, they are not particularly applicable to in-situ analysis and thus focus is placed exclusively on the patterns presented in this chapter as being the most applicable. The next chapter serves to illustrate this further with the use of visualizations applied to analysis jobs that fit many of the patterns and categories previously described, and some additional discussion of difficulties and potential for custom visualizations. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[]
\centering
\caption{Most applicable visualizations for each pattern-category task}
\label{tbl:usages}
\begin{tabular}{l|llll}
                  & Bar Chart & Line Chart & Scatter Plot & Custom \\ \hline
Summarization     & \checkmark& \checkmark &              &        \\
Filtering         & \checkmark& \checkmark & \checkmark   &        \\
Data Organization &           &            &              & \checkmark \\
Joins             &           &            &              & \checkmark       \\
Meta Patterns     & \checkmark& \checkmark & \checkmark   & \checkmark      
\end{tabular}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



  

